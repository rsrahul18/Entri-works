# -*- coding: utf-8 -*-
"""regression assignment final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AFcIXziet414353Z-Ce_M2s8uv8VWs92
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

#Loading and preprocessing
from sklearn.datasets import fetch_california_housing
cal = fetch_california_housing(as_frame=True)
cal

df = cal.frame
df

df.info()

df.describe()

nu=df.select_dtypes(include=['number'])
nu

nu.skew()

#AveRooms
Q1 = df['AveRooms'].quantile(0.25)
Q3 = df['AveRooms'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df['AveRooms'] = np.clip(df['AveRooms'], lower_bound, upper_bound)  # Cap outlier

#AveBedrms
Q1 = df['AveBedrms'].quantile(0.25)
Q3 = df['AveBedrms'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
df['AveBedrms'] = np.clip(df['AveBedrms'], lower_bound, upper_bound)  # Cap outlier

df.describe()

#feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df.drop(columns=["MedHouseVal"]))# Scaling all features except target

df["RoomsPerHousehold"] = df["AveRooms"] / df["AveOccup"]
df["BedrmsPerRoom"] = df["AveBedrms"] / df["AveRooms"]

df

X = df.drop(columns=["MedHouseVal"])
y = df["MedHouseVal"]

#train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#1. Linear regression - Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a straight line.
#2.Decision tree regressor-A Decision Tree Regressor is a machine learning model that predicts continuous values by recursively splitting the data into branches based on feature values.
#3.Random forest regressor-A Random Forest Regressor is an ensemble learning model that predicts continuous values by averaging outputs from multiple decision trees to improve accuracy and reduce overfitting.
#4.Radient boosting regressor-A Gradient Boosting Regressor is an ensemble learning model that builds sequential decision trees to minimize prediction errors by correcting previous mistakes.
#5.Support vector regressor-A Support Vector Regressor (SVR)is a machine learning model that uses Support Vector Machines to predict continuous values by finding a hyperplane that best fits the data within a specified margin.

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Initialize models
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "Support Vector Regressor": SVR()
}

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train and evaluate models
results = {}

for name, model in models.items():
    print(f"Training {name}...")  # Debugging statement

    try:
        if name == "Support Vector Regressor":
            model.fit(X_train_scaled, y_train)  # Train with scaled features
            y_pred = model.predict(X_test_scaled)  # Predict with scaled features
        else:
            model.fit(X_train, y_train)  # Train on original features
            y_pred = model.predict(X_test)  # Predict on original features

        # Calculate performance metrics
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        results[name] = {"MSE": mse, "MAE": mae, "R²": r2}

        print(f"✅ {name} completed.")  # Debugging statement

    except Exception as e:
        print(f"❌ Error in {name}: {e}")

# Convert results to DataFrame for easy comparison
results_df = pd.DataFrame(results).T
print("\nFinal Results:\n", results_df)

# Model Performance Analysis
#We compare models using three key metrics:

#MSE (Mean Squared Error) → Lower is better ✅
#MAE (Mean Absolute Error) → Lower is better ✅
#R² (R-squared Score) → Higher is better ✅

#Best Performing Model: Random Forest Regressor
 # Lowest MSE → 0.2567 (best at minimizing squared errors)
# Lowest MAE → 0.3297 (best at minimizing absolute errors)
# Highest R² → 0.8041 (best at explaining variance in data)

#This means Random Forest Regressor makes the most accurate predictions with the least error.

#Secondary Option: Gradient Boosting Regressor
#Slightly higher error than Random Forest.
#Performs better than other models but slightly worse than Random Forest.
#Could be preferred if interpretability or computational efficiency is needed.