# -*- coding: utf-8 -*-
"""classification assignment ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l4Dj1laT3cxx1HoAiQI_bt7LKRNLVqbo
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#load the dataset
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
x=data.data
y=data.target

df=pd.DataFrame(x,columns=data.feature_names)
df['target']=y

df

df.info()

#feature scaling
#standardscaler
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
#fit the scalar on data
scaler.fit(df)
#transform the data
scaled_data=scaler.transform(df)
#print the scaled data
scaled_data

#minmaxscaler
from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
#fit the scalar on data
scaler.fit(df)
#transform the data
scaled_data=scaler.transform(df)
#print the scaled data
scaled_data

#define features and target
x=df.drop('target',axis=1)
y=df['target']

x

y

#labelencoder
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
y=le.fit_transform(y)
y

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

#split the dataset into training and splitting sets
x_train,x_test,y_train,y_test = train_test_split(x,y)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

#Initialize and train the model
logreg=LogisticRegression()
logreg.fit(x_train,y_train)

#make prediction
y_pred=logreg.predict(x_test)
y_pred

y_test

#accuracy score
accuracy_score=accuracy_score(y_test,y_pred)
accuracy_score

#classification report
cr=classification_report(y_test,y_pred)
print(cr)

#confusion matrix
cm=confusion_matrix(y_test,y_pred)
cm

#heatmap for confusion matrix
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm,annot=True,xticklabels=le.classes_,yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Truth')
plt.show()

#Knn
from sklearn.neighbors import KNeighborsClassifier
#create a kneighbors classifier
knn=KNeighborsClassifier(n_neighbors=3)

#Fit the model
knn.fit(x_train,y_train)

#predict on the test set
knn_y_pred=knn.predict(x_test)
knn_y_pred

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score=accuracy_score(y_test,knn_y_pred)
accuracy_score

#Desicion tree
from sklearn.tree import DecisionTreeClassifier
dt=DecisionTreeClassifier()
dt.fit(x_train,y_train)
#make prediction
dt_y_pred=dt.predict(x_test)
dt_y_pred

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score=accuracy_score(y_test,dt_y_pred)
accuracy_score

#Random forest classifier
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(x_train,y_train)
#make prediction
rf_y_pred=rf.predict(x_test)
rf_y_pred

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
accuracy_score=accuracy_score(y_test,rf_y_pred)
accuracy_score

#Support vector classifier
from sklearn.svm import SVC
svc=SVC()
svc.fit(x_train,y_train)
#make predictions
svc_y_pred=svc.predict(x_test)

svc_y_pred

import sklearn.metrics as metrics
print(metrics.accuracy_score(y_test,svc_y_pred))

#FINAL OUTPUT
#LOGISTIC REGRESSION ACCURACY =0.9370629370629371
#KNN ACCURACY                 =0.9020979020979021
#DECISION TREE ACCURACY       =0.9230769230769231
#RANDOM FOREST ACCURACY       =0.958041958041958
#SUPPORT VECTOR ACCURACY      =0.951048951048951

#MODEL COMPARISON
#from this data Random forest classifier and support vector classifier are the best models which have a accuracy score above 0.95 .so we can select both models for predict a unseen data.
#we canot predict the worst model because all model have accurcy score above or equal to 0.90 .so all models are good .